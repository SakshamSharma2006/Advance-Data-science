{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q0KcxGLJsKY"
      },
      "source": [
        "# Attention Mechanism in Sequence-to-Sequence (Seq2Seq) Models\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates the implementation of an attention mechanism in a Seq2Seq model for machine translation. The attention mechanism allows the decoder to focus on different parts of the input sequence when generating each output token.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Encoder**: Processes input sequence and produces context vectors\n",
        "- **Decoder**: Generates output sequence using attention over encoder outputs\n",
        "- **Attention Mechanism**: Dynamically weights encoder outputs to create context for each decoder step\n",
        "- **Attention Weights**: Softmax-normalized scores indicating focus on each input word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrGFznQlJsKa"
      },
      "source": [
        "## Part 1: Import Libraries and Setup\n",
        "\n",
        "We'll start by importing all necessary libraries for building and training our attention-based Seq2Seq model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvCJRIweJsKa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Set device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIwxlTLuJsKb"
      },
      "source": [
        "## Part 2: Dataset Preparation\n",
        "\n",
        "We'll use a small English-to-French translation dataset. In practice, you'd use larger datasets from sources like Multi30k or WMT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0myoILKJsKb"
      },
      "outputs": [],
      "source": [
        "# Simple English-French translation pairs\n",
        "pairs = [\n",
        "    (\"i am a student\", \"je suis etudiant\"),\n",
        "    (\"he is a teacher\", \"il est professeur\"),\n",
        "    (\"she is a doctor\", \"elle est medecin\"),\n",
        "    (\"we are engineers\", \"nous sommes ingenieurs\"),\n",
        "    (\"they are artists\", \"ils sont artistes\")\n",
        "]\n",
        "\n",
        "print(\"Dataset:\")\n",
        "for eng, fra in pairs:\n",
        "    print(f\"  EN: {eng:30s} | FR: {fra}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2rjPYdxJsKb"
      },
      "source": [
        "## Part 3: Language Vocabulary Class\n",
        "\n",
        "This class builds a vocabulary by mapping words to indices. Special tokens:\n",
        "- `<SOS>` (Start Of Sequence): Index 0, marks the beginning of a sentence\n",
        "- `<EOS>` (End Of Sequence): Index 1, marks the end of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPO8IGQsJsKb"
      },
      "outputs": [],
      "source": [
        "class Lang:\n",
        "    \"\"\"Class to manage vocabulary and word-to-index mapping.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize with special tokens.\"\"\"\n",
        "        self.word2index = {\"<SOS>\": 0, \"<EOS>\": 1}\n",
        "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\"}\n",
        "        self.n_words = 2  # Count of unique words\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        \"\"\"Add all words from a sentence to vocabulary.\"\"\"\n",
        "        for word in sentence.split(' '):\n",
        "            if word not in self.word2index:\n",
        "                self.word2index[word] = self.n_words\n",
        "                self.index2word[self.n_words] = word\n",
        "                self.n_words += 1\n",
        "\n",
        "# Build vocabularies for both languages\n",
        "input_lang, output_lang = Lang(), Lang()\n",
        "\n",
        "for eng, fra in pairs:\n",
        "    input_lang.add_sentence(eng)\n",
        "    output_lang.add_sentence(fra)\n",
        "\n",
        "print(f\"English Vocabulary Size: {input_lang.n_words}\")\n",
        "print(f\"French Vocabulary Size: {output_lang.n_words}\")\n",
        "print(f\"\\nEnglish Word2Index: {input_lang.word2index}\")\n",
        "print(f\"\\nFrench Word2Index: {output_lang.word2index}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5WL9MZbJsKb"
      },
      "source": [
        "## Part 4: Encoder Architecture\n",
        "\n",
        "The **Encoder** reads the input sequence and produces:\n",
        "- **Output**: Hidden state at each timestep (used for attention)\n",
        "- **Hidden State**: Final context vector passed to decoder\n",
        "\n",
        "### Components:\n",
        "- **Embedding Layer**: Converts word indices to dense vectors\n",
        "- **GRU Layer**: Gated Recurrent Unit processes embeddings sequentially"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy8pAXR-JsKc"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder: Maps input sequence to hidden representations.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        \"\"\"Initialize encoder layers.\n",
        "\n",
        "        Args:\n",
        "            input_size: Size of input vocabulary\n",
        "            hidden_size: Dimension of hidden states\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Embedding layer: converts word indices to vectors\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        # GRU layer: processes sequence of embeddings\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\"Forward pass through encoder.\n",
        "\n",
        "        Args:\n",
        "            input: Word index tensor\n",
        "            hidden: Previous hidden state\n",
        "\n",
        "        Returns:\n",
        "            output: Current output (used for attention)\n",
        "            hidden: New hidden state\n",
        "        \"\"\"\n",
        "        # Embed the input word\n",
        "        embedded = self.embedding(input).view(1, 1, -1)  # Shape: (1, 1, hidden_size)\n",
        "\n",
        "        # Pass through GRU\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "print(\"Encoder class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZBUgBLFJsKc"
      },
      "source": [
        "## Part 5: Attention Decoder Architecture\n",
        "\n",
        "The **Attention Decoder** generates the output sequence while dynamically attending to input words.\n",
        "\n",
        "### Attention Mechanism Steps:\n",
        "1. **Attention Score**: Combine decoder hidden state with encoder output\n",
        "2. **Attention Weights**: Apply softmax to get probability distribution over inputs\n",
        "3. **Context Vector**: Weighted sum of encoder outputs\n",
        "4. **Combine**: Concatenate context with decoder input and pass through GRU\n",
        "5. **Output**: Linear layer produces probability distribution over target vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBkbwb8NJsKc"
      },
      "outputs": [],
      "source": [
        "class AttnDecoder(nn.Module):\n",
        "    \"\"\"Decoder with Attention mechanism.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, max_length=10):\n",
        "        \"\"\"Initialize decoder layers.\n",
        "\n",
        "        Args:\n",
        "            hidden_size: Dimension of hidden states\n",
        "            output_size: Size of output vocabulary\n",
        "            max_length: Maximum sequence length for attention\n",
        "        \"\"\"\n",
        "        super(AttnDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Embedding layer for target vocabulary\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "\n",
        "        # Attention layer: computes attention scores\n",
        "        # Input: concatenation of embedding and hidden state\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "\n",
        "        # Attention combine layer: combines context and embedding\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "\n",
        "        # Output projection: maps hidden state to vocabulary probabilities\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        \"\"\"Forward pass with attention.\n",
        "\n",
        "        Args:\n",
        "            input: Word index for decoder input\n",
        "            hidden: Decoder hidden state\n",
        "            encoder_outputs: All encoder outputs for attention\n",
        "\n",
        "        Returns:\n",
        "            output: Probabilities over output vocabulary\n",
        "            hidden: New hidden state\n",
        "            attn_weights: Attention weights\n",
        "        \"\"\"\n",
        "        # Step 1: Embed the input\n",
        "        embedded = self.embedding(input).view(1, 1, -1)  # Shape: (1, 1, hidden_size)\n",
        "\n",
        "        # Step 2: Calculate attention weights\n",
        "        # Concatenate embedded input with current hidden state\n",
        "        attn_input = torch.cat((embedded[0], hidden[0]), 1)  # Shape: (1, hidden_size*2)\n",
        "        attn_scores = self.attn(attn_input)  # Shape: (1, max_length)\n",
        "        attn_weights = F.softmax(attn_scores, dim=1)  # Normalize to probabilities\n",
        "\n",
        "        # Step 3: Apply attention weights to encoder outputs\n",
        "        # This creates a weighted context vector\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        # Step 4: Combine embedded input with attention context\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)  # Shape: (1, hidden_size*2)\n",
        "        output = self.attn_combine(output).unsqueeze(0)  # Shape: (1, 1, hidden_size)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        # Step 5: Pass through GRU\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        # Step 6: Generate output probabilities\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "print(\"Attention Decoder class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klC4mnZQJsKc"
      },
      "source": [
        "## Part 6: Training Function\n",
        "\n",
        "The training function implements the full forward and backward pass:\n",
        "\n",
        "### Training Steps:\n",
        "1. **Encode**: Process entire input sequence, storing all outputs\n",
        "2. **Decode with Teacher Forcing**: Feed ground truth as input at each step\n",
        "3. **Calculate Loss**: Compare predictions with ground truth\n",
        "4. **Backpropagate**: Update model parameters via gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky_wViQQJsKc"
      },
      "outputs": [],
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder,\n",
        "          encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
        "    \"\"\"Train the encoder and decoder for one sample.\n",
        "\n",
        "    Args:\n",
        "        input_tensor: Input sequence as tensor\n",
        "        target_tensor: Target sequence as tensor\n",
        "        encoder: Encoder model\n",
        "        decoder: Decoder model with attention\n",
        "        encoder_optimizer: Optimizer for encoder\n",
        "        decoder_optimizer: Optimizer for decoder\n",
        "        criterion: Loss function (NLLLoss)\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        Normalized loss\n",
        "    \"\"\"\n",
        "    # Initialize encoder hidden state\n",
        "    encoder_hidden = torch.zeros(1, 1, encoder.hidden_size, device=device)\n",
        "\n",
        "    # Clear gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # Store encoder outputs for attention\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # ENCODER: Process input sequence word by word\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]  # Store for attention\n",
        "\n",
        "    # DECODER: Generate output sequence with teacher forcing\n",
        "    # Start with SOS (Start of Sequence) token\n",
        "    decoder_input = torch.tensor([[0]], device=device)\n",
        "    decoder_hidden = encoder_hidden  # Pass encoder's final state to decoder\n",
        "\n",
        "    # Teacher forcing: feed ground truth at each step\n",
        "    for di in range(target_length):\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "        # Calculate loss for this step\n",
        "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
        "\n",
        "        # Use ground truth as next input (teacher forcing)\n",
        "        decoder_input = target_tensor[di]\n",
        "\n",
        "    # BACKPROPAGATION\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "print(\"Training function defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHeUaanwJsKc"
      },
      "source": [
        "## Part 7: Model Initialization\n",
        "\n",
        "Now we'll initialize the encoder and decoder models with hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ht0BbrVJsKc"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 128\n",
        "learning_rate = 0.01\n",
        "num_epochs = 400\n",
        "\n",
        "# Initialize models\n",
        "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoder(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "# Initialize optimizers\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "# Loss function (for log probabilities)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "print(f\"Encoder: {encoder}\")\n",
        "print(f\"\\nDecoder: {decoder}\")\n",
        "print(f\"\\nTotal encoder parameters: {sum(p.numel() for p in encoder.parameters())}\")\n",
        "print(f\"Total decoder parameters: {sum(p.numel() for p in decoder.parameters())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmyaQFK5JsKd"
      },
      "source": [
        "## Part 8: Training Loop\n",
        "\n",
        "Train the model for multiple epochs, iterating over all translation pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBUWMw5MJsKd"
      },
      "outputs": [],
      "source": [
        "print(f\"Starting training for {num_epochs} epochs...\\n\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for eng, fra in pairs:\n",
        "        # Convert sentences to tensor of word indices\n",
        "        input_tensor = torch.tensor(\n",
        "            [input_lang.word2index[w] for w in eng.split(' ')],\n",
        "            device=device\n",
        "        )\n",
        "        target_tensor = torch.tensor(\n",
        "            [output_lang.word2index[w] for w in fra.split(' ')],\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Train on this pair\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder,\n",
        "                    encoder_optimizer, decoder_optimizer, criterion)\n",
        "        total_loss += loss\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        avg_loss = total_loss / len(pairs)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TPXFJ_CJsKd"
      },
      "source": [
        "## Part 9: Inference Function\n",
        "\n",
        "This function translates a sentence without teacher forcing (greedy decoding).\n",
        "\n",
        "### Decoding Strategy:\n",
        "- **Greedy Decoding**: Select word with highest probability at each step\n",
        "- **Attention Visualization**: Collect attention weights for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mai8-FrDJsKd"
      },
      "outputs": [],
      "source": [
        "def translate(encoder, decoder, sentence, max_length=10):\n",
        "    \"\"\"Translate a sentence without teacher forcing.\n",
        "\n",
        "    Args:\n",
        "        encoder: Trained encoder\n",
        "        decoder: Trained decoder\n",
        "        sentence: Input sentence string\n",
        "        max_length: Maximum output length\n",
        "\n",
        "    Returns:\n",
        "        translated_words: List of translated words\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Convert input sentence to tensor\n",
        "        input_tensor = torch.tensor(\n",
        "            [input_lang.word2index[w] for w in sentence.split(' ')],\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Initialize encoder\n",
        "        encoder_hidden = torch.zeros(1, 1, encoder.hidden_size, device=device)\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        # Encode input\n",
        "        for i in range(input_tensor.size(0)):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "            encoder_outputs[i] = encoder_output[0, 0]\n",
        "\n",
        "        # Initialize decoder with SOS token\n",
        "        decoder_input = torch.tensor([[0]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        translated_words = []\n",
        "\n",
        "        # Generate output greedily\n",
        "        for _ in range(max_length):\n",
        "            decoder_output, decoder_hidden, attn_weights = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "\n",
        "            # Select word with highest probability\n",
        "            topi = decoder_output.argmax(1)\n",
        "\n",
        "            # Stop if EOS token is generated\n",
        "            if topi.item() == 1:\n",
        "                break\n",
        "\n",
        "            # Add word to translation\n",
        "            translated_words.append(output_lang.index2word[topi.item()])\n",
        "            decoder_input = topi.view(1, 1)\n",
        "\n",
        "        return translated_words\n",
        "\n",
        "# Test translation\n",
        "print(\"Translation Examples:\")\n",
        "for eng, fra in pairs:\n",
        "    translated = translate(encoder, decoder, eng)\n",
        "    print(f\"EN: {eng:30s} | FR (True): {fra:30s} | FR (Predicted): {' '.join(translated)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oprl1y_1JsKd"
      },
      "source": [
        "## Part 10: Visualization - Attention Heatmap\n",
        "\n",
        "Now we'll visualize the attention weights as a heatmap. This shows which input words the decoder focuses on when generating each output word.\n",
        "\n",
        "### Interpretation:\n",
        "- **X-axis**: Input words (English)\n",
        "- **Y-axis**: Output words (French)\n",
        "- **Color intensity**: Attention weight (lighter = higher attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTA1de3KJsKd"
      },
      "outputs": [],
      "source": [
        "def visualize_attention(encoder, decoder, sentence, max_length=10):\n",
        "    \"\"\"Visualize attention weights as a heatmap.\n",
        "\n",
        "    Args:\n",
        "        encoder: Trained encoder\n",
        "        decoder: Trained decoder\n",
        "        sentence: Input sentence to visualize\n",
        "        max_length: Maximum sequence length\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Convert input to tensor\n",
        "        input_tensor = torch.tensor(\n",
        "            [input_lang.word2index[word] for word in sentence.split(' ')],\n",
        "            device=device\n",
        "        )\n",
        "        input_length = input_tensor.size(0)\n",
        "\n",
        "        # Initialize encoder\n",
        "        encoder_hidden = torch.zeros(1, 1, encoder.hidden_size, device=device)\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        # Encode input\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "        # Initialize decoder\n",
        "        decoder_input = torch.tensor([[0]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        # Generate output and collect attention weights\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "\n",
        "            # Get word with highest probability\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == 1:  # EOS\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        # Trim attention matrix to actual lengths\n",
        "        attention_data = decoder_attentions[:len(decoded_words), :input_length].cpu().numpy()\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Plot heatmap\n",
        "        im = ax.imshow(attention_data, cmap='bone', aspect='auto')\n",
        "        cbar = fig.colorbar(im, ax=ax)\n",
        "        cbar.set_label('Attention Weight', rotation=270, labelpad=20)\n",
        "\n",
        "        # Set ticks and labels\n",
        "        ax.set_xticks(np.arange(input_length))\n",
        "        ax.set_yticks(np.arange(len(decoded_words)))\n",
        "        ax.set_xticklabels(sentence.split(' '), rotation=45, ha='right')\n",
        "        ax.set_yticklabels(decoded_words)\n",
        "\n",
        "        # Add grid\n",
        "        ax.set_xticks(np.arange(input_length) - 0.5, minor=True)\n",
        "        ax.set_yticks(np.arange(len(decoded_words)) - 0.5, minor=True)\n",
        "        ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n",
        "\n",
        "        # Labels\n",
        "        ax.set_xlabel('Input Words (English)', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('Output Words (French)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            f'Attention Mechanism: \"{sentence}\" â†’ \"{\" \".join(decoded_words)}\"',\n",
        "            fontsize=13, fontweight='bold', pad=15\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return decoded_words, attention_data\n",
        "\n",
        "print(\"Visualization function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIWxz7S1JsKd"
      },
      "source": [
        "## Part 11: Visualize Attention for Sample Sentences\n",
        "\n",
        "Let's see how the attention mechanism works for different input sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiPjKvZKJsKd"
      },
      "outputs": [],
      "source": [
        "# Visualize attention for different sentences\n",
        "print(\"=\"*60)\n",
        "print(\"ATTENTION VISUALIZATION FOR SENTENCE 1\")\n",
        "print(\"=\"*60)\n",
        "visualize_attention(encoder, decoder, \"i am a student\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ATTENTION VISUALIZATION FOR SENTENCE 2\")\n",
        "print(\"=\"*60)\n",
        "visualize_attention(encoder, decoder, \"he is a teacher\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ATTENTION VISUALIZATION FOR SENTENCE 3\")\n",
        "print(\"=\"*60)\n",
        "visualize_attention(encoder, decoder, \"she is a doctor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot-S8IZ4JsKd"
      },
      "source": [
        "## Part 12: Enhanced Visualization with Seaborn\n",
        "\n",
        "Alternative visualization using seaborn for a more polished look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofT3YSr3JsKd"
      },
      "outputs": [],
      "source": [
        "def visualize_attention_seaborn(encoder, decoder, sentence, max_length=10):\n",
        "    \"\"\"Visualize attention using seaborn heatmap.\n",
        "\n",
        "    Args:\n",
        "        encoder: Trained encoder\n",
        "        decoder: Trained decoder\n",
        "        sentence: Input sentence\n",
        "        max_length: Maximum sequence length\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor(\n",
        "            [input_lang.word2index[word] for word in sentence.split(' ')],\n",
        "            device=device\n",
        "        )\n",
        "        input_length = input_tensor.size(0)\n",
        "\n",
        "        encoder_hidden = torch.zeros(1, 1, encoder.hidden_size, device=device)\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[0]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == 1:\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        attention_data = decoder_attentions[:len(decoded_words), :input_length].cpu().numpy()\n",
        "\n",
        "        # Seaborn heatmap\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        sns.heatmap(\n",
        "            attention_data,\n",
        "            xticklabels=sentence.split(' '),\n",
        "            yticklabels=decoded_words,\n",
        "            cmap='YlOrRd',\n",
        "            cbar_kws={'label': 'Attention Weight'},\n",
        "            ax=ax,\n",
        "            linewidths=0.5,\n",
        "            linecolor='gray'\n",
        "        )\n",
        "\n",
        "        ax.set_xlabel('Input Words (English)', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('Output Words (French)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            f'Attention Heatmap (Seaborn): \"{sentence}\"',\n",
        "            fontsize=13, fontweight='bold'\n",
        "        )\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return decoded_words, attention_data\n",
        "\n",
        "# Visualize with seaborn\n",
        "print(\"SEABORN VISUALIZATION\")\n",
        "visualize_attention_seaborn(encoder, decoder, \"we are engineers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcEp9cTpJsKd"
      },
      "source": [
        "## Part 13: Summary and Key Insights\n",
        "\n",
        "### What We Learned:\n",
        "\n",
        "1. **Encoder-Decoder Architecture**: The encoder processes the input and passes a context vector to the decoder\n",
        "\n",
        "2. **Attention Mechanism**:\n",
        "   - Calculates dynamic attention weights for each decoder step\n",
        "   - Allows the model to focus on relevant input words\n",
        "   - Improves translation quality, especially for longer sequences\n",
        "\n",
        "3. **Training Process**:\n",
        "   - Uses teacher forcing during training\n",
        "   - Loss is calculated for each output word\n",
        "   - Gradients flow through both encoder and decoder\n",
        "\n",
        "4. **Inference**:\n",
        "   - Uses greedy decoding (select highest probability word)\n",
        "   - No teacher forcing at inference time\n",
        "   - Attention weights show interpretability\n",
        "\n",
        "### Attention Weights Interpretation:\n",
        "- The heatmap shows how much the decoder attends to each input word\n",
        "- Lighter colors = higher attention\n",
        "- Typically shows alignment between source and target languages\n",
        "- Can reveal linguistic patterns the model has learned\n",
        "\n",
        "### Extensions:\n",
        "- **Multi-head Attention**: Multiple attention mechanisms in parallel\n",
        "- **Transformer Models**: Replaced RNNs with self-attention\n",
        "- **Beam Search**: Generate multiple hypotheses for better translations\n",
        "- **Larger Datasets**: Use real datasets (WMT, Multi30k) for better performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozre0ecFJsKe"
      },
      "outputs": [],
      "source": [
        "# Final statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Encoder hidden size: {hidden_size}\")\n",
        "print(f\"Encoder parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
        "print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters()):,}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in encoder.parameters()) + sum(p.numel() for p in decoder.parameters()):,}\")\n",
        "print(f\"\\nTraining epochs: {num_epochs}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}