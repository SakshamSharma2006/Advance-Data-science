{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning MLP Document Summary\n",
        "\n",
        "## Overview\n",
        "This document presents two comprehensive deep learning implementations using fully connected neural networks (MLPs) and CNNs for image classification tasks, incorporating various regularization techniques, optimization methods, and best practices.\n",
        "\n",
        "## Project 1: CIFAR-10 MLP Classification\n",
        "\n",
        "### Dataset Used\n",
        "- **CIFAR-10**: 60,000 32x32 color images across 10 categories (airplane, car, bird, cat, dog, etc.)\n",
        "- **Preprocessing**: Images normalized to range [0, 1] and labels converted to one-hot encoding\n",
        "\n",
        "### Model Architecture\n",
        "- **Input Layer**: Flattened 32x32x3 images into 1D vectors (3072 features)\n",
        "- **Hidden Layers**: 3 fully connected (dense) layers\n",
        "  - Layer 1: 512 units with ReLU activation\n",
        "  - Layer 2: 256 units with ReLU activation  \n",
        "  - Layer 3: 128 units with ReLU activation\n",
        "- **Output Layer**: 10 units with Softmax activation for multi-class classification\n",
        "\n",
        "### Regularization Techniques\n",
        "- **L2 Regularization**: Applied to dense layers (Î» = 0.001) to penalize large weights\n",
        "- **Dropout**: 0.5 dropout rate after each dense layer to prevent overfitting\n",
        "- **Early Stopping**: Stops training after 5 epochs without validation improvement\n",
        "\n",
        "### Optimization Strategy\n",
        "- **Primary Optimizer**: Adam with learning rate = 0.0001\n",
        "- **Learning Rate Scheduler**: ReduceLROnPlateau (reduces LR by factor 0.2 when validation loss plateaus)\n",
        "- **Alternative Optimizers**: SGD and RMSprop also supported\n",
        "\n",
        "### Training Configuration\n",
        "- **Epochs**: Maximum 50 epochs\n",
        "- **Batch Size**: 64\n",
        "- **Validation**: Uses test set for validation during training\n",
        "\n",
        "### Callbacks Used\n",
        "- **EarlyStopping**: Monitors validation loss with patience=5\n",
        "- **ModelCheckpoint**: Saves best model based on validation loss\n",
        "- **ReduceLROnPlateau**: Dynamic learning rate adjustment\n",
        "\n",
        "### Performance Results\n",
        "- **Test Accuracy**: 46.58%\n",
        "- **Training completed**: 50 epochs with learning rate reduction at epoch 35\n",
        "\n"
      ],
      "metadata": {
        "id": "awiV0Nx8v0g-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyLe4-vnrDrV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data to range [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define MLP model architecture\n",
        "def create_mlp_model(optimizer='adam', dropout_rate=0.5, regularizer=None):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Flatten the input image (32x32x3) to a 1D vector (3072)\n",
        "    model.add(layers.Flatten(input_shape=(32, 32, 3)))\n",
        "\n",
        "    # First dense layer\n",
        "    model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizer))\n",
        "    model.add(layers.Dropout(dropout_rate))  # Dropout regularization\n",
        "\n",
        "    # Second dense layer\n",
        "    model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizer))\n",
        "    model.add(layers.Dropout(dropout_rate))  # Dropout regularization\n",
        "\n",
        "    # Third dense layer\n",
        "    model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizer))\n",
        "    model.add(layers.Dropout(dropout_rate))  # Dropout regularization\n",
        "\n",
        "    # Output layer with softmax activation for classification\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model with the specified optimizer\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_mlp_model.keras', monitor='val_loss', save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Choose regularization (L1, L2, L1_L2)\n",
        "regularizer = regularizers.l2(0.001)\n",
        "\n",
        "# Choose optimizer (SGD, Adam, RMSprop)\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "# Create the MLP model\n",
        "model = create_mlp_model(optimizer=optimizer, dropout_rate=0.5, regularizer=regularizer)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=50,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[early_stopping, checkpoint, reduce_lr])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Visualize the training process\n",
        "# Accuracy plot\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Model Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# Loss plot\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Model Loss')\n",
        "plt.show()"
      ]
    }
  ]
}