{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST CNN Classification\n",
        "\n",
        "### Dataset Used\n",
        "- **MNIST**: 28x28 grayscale images of handwritten digits (0-9)\n",
        "- **Preprocessing**: Normalized to [0, 1] range and reshaped for Conv2D layers\n",
        "\n",
        "### Model Architecture\n",
        "- **Convolutional Layers**:\n",
        "  - Conv2D: 32 filters, 3x3 kernel, ReLU activation\n",
        "  - Conv2D: 64 filters, 3x3 kernel, ReLU activation\n",
        "  - MaxPooling2D: 2x2 pooling\n",
        "- **Dense Layers**:\n",
        "  - Flatten layer\n",
        "  - Dense: 128 units with ReLU activation\n",
        "  - Dense: 10 units with Softmax activation (output)\n",
        "\n",
        "### Regularization Techniques\n",
        "- **L2 Regularization**: Applied to dense layers (Î» = 0.01)\n",
        "- **Dropout**: 0.3 dropout rate applied after conv layers and dense layers\n",
        "- **Batch Normalization**: Applied after each convolutional layer\n",
        "- **Early Stopping**: Patience=3 epochs\n",
        "\n",
        "### Optimization Strategy\n",
        "- **Primary Optimizer**: Adam (default settings)\n",
        "- **Alternative Optimizers**: SGD and RMSprop supported\n",
        "\n",
        "### Training Configuration\n",
        "- **Epochs**: 5 (recommended 20 for better results)\n",
        "- **Batch Size**: 64\n",
        "- **Validation**: Uses test set for validation\n",
        "\n",
        "### Callbacks Used\n",
        "- **EarlyStopping**: Monitors validation loss with patience=3\n",
        "- **ModelCheckpoint**: Saves best model to 'best_model.keras'\n",
        "\n",
        "### Performance Results\n",
        "- **Test Accuracy**: 98.30%\n",
        "- **Training completed**: 5 epochs with excellent convergence\n",
        "\n",
        "## Key Features Across Both Projects\n",
        "\n",
        "### Activation Functions\n",
        "- **ReLU**: Used in hidden layers for non-linearity\n",
        "- **Softmax**: Used in output layer for probability distribution\n",
        "\n",
        "### Regularization Options\n",
        "- **L1, L2, L1_L2**: Configurable regularization types\n",
        "- **Dropout**: Configurable dropout rates\n",
        "- **Batch Normalization**: For training stability (CNN only)\n",
        "\n",
        "### Visualization\n",
        "- **Training Plots**: Accuracy and loss curves for both training and validation\n",
        "- **Performance Monitoring**: Real-time tracking of model performance\n",
        "\n",
        "### Customization Capabilities\n",
        "- **Flexible Architecture**: Easily modifiable layer sizes and depths\n",
        "- **Optimizer Selection**: Multiple optimization algorithms supported\n",
        "- **Hyperparameter Tuning**: Configurable learning rates, regularization strengths\n",
        "- **Regularization Combinations**: Mix and match different regularization techniques\n",
        "\n",
        "## Technical Highlights\n",
        "- **Framework**: TensorFlow/Keras\n",
        "- **Model Saving**: Automatic best model checkpointing\n",
        "- **Robust Training**: Early stopping prevents overfitting\n",
        "- **Adaptive Learning**: Dynamic learning rate adjustment\n",
        "- **Comprehensive Evaluation**: Detailed performance metrics and visualizations"
      ],
      "metadata": {
        "id": "arbrTccuwI8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R0R5NJTs6ZV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)  # Reshape for Conv2D layer\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the model\n",
        "def create_model(optimizer='adam', regularizer=None, dropout_rate=0.5):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # First Conv layer with activation and BatchNormalization\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Second Conv layer\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(dropout_rate))  # Dropout regularization\n",
        "\n",
        "    # Flatten before dense layers\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense layers with L2 regularization\n",
        "    model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizer))\n",
        "    model.add(layers.Dropout(dropout_rate))  # Dropout regularization\n",
        "\n",
        "    # Output layer with softmax activation\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile model with chosen optimizer\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Choose regularizer (L1, L2, L1_L2)\n",
        "regularizer = regularizers.l2(0.01)\n",
        "\n",
        "# Choose optimizer (SGD, Adam, RMSprop)\n",
        "optimizer = Adam()\n",
        "\n",
        "# Create the model\n",
        "model = create_model(optimizer=optimizer, regularizer=regularizer, dropout_rate=0.3)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=5,  # use 20 epochs for better results\n",
        "                    batch_size=64,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Optionally, you can visualize the training history or predictions\n",
        "# Plot accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ]
}
